{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence to Sequence Learning with Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXs2C4Yw4VDkLTurUEyt28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_v4vGrP_QF1"
      },
      "source": [
        "[논문 : Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "  \n",
        "코드참고 : [케라스 블로그](https://keras.io/examples/nlp/lstm_seq2seq/)\n",
        "\n",
        "\n",
        "> 진행 과정\n",
        "1. 문장을 (encoder_input_data, decoder_input_data, decoder_target_data)의 3차원 배열로 변경. \n",
        "encoder_input_data : 3차원형태, 영어문장의 one-hot벡터화 형식의 데이터를 포함  \n",
        "decoder_input_data : 3차원형태, 프랑스어문장의 one-hot벡터화 형식의 데이터를 포함  \n",
        "decoder_target_data : decoder_input_data와 같으나 1타임스텝만큼 오프셋  \n",
        "2. 기본 LSTM 기반의 Seq2Seq모델을 주어진 encoder, decoder input data로 decoder_target_data를 예측. teacher forcing을 사용한다.   \n",
        "3. 모델이 잘 작동하는지 몇몇의 문장을 decode.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxtA57Tz_66G"
      },
      "source": [
        "## 모듈 임포트 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siPXyU18-o8X"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaUsxP5AFeX"
      },
      "source": [
        "## 데이터 다운로드  \n",
        "원본 논문대로 영어 - 프랑스어 데이터 셋 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW9XWbclABVc",
        "outputId": "0d6425bc-5dc5-48c6-d219-cccb782649c4"
      },
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  fra-eng.zip',\n",
              " '  inflating: _about.txt              ',\n",
              " '  inflating: fra.txt                 ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILQO8CaAAMr-"
      },
      "source": [
        "## 하이퍼 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGnkDMIjAHAI"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = 'fra.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EibHbEacC5sn"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbHwSuBlC2Ok",
        "outputId": "55ca1721-b94a-4714-b31f-0edc755766c5"
      },
      "source": [
        "# 데이터 벡터화\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path,\"r\",encoding='utf-8') as f:\n",
        "  lines = f.read().split(\"\\n\")\n",
        "\n",
        "for line in lines[: min(num_samples,len(lines) -1)]:\n",
        "  input_text, target_text,_ = line.split(\"\\t\")\n",
        "  # 탭을 시퀀스를 시작하는 문자로 사용. \\n을 끝 시퀀스 문자로 사용\n",
        "  target_text = '\\t' + target_text + \"\\n\"\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "\n",
        "  for char in input_text:\n",
        "    if char not in input_characters:\n",
        "      input_characters.add(char)\n",
        "  for char in target_text:\n",
        "    if char not in target_characters:\n",
        "      target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data는 decoder_input_data에 비해서 1타임스텝 앞서있다.\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data는 시작문자를 포함하지 않으며, 1타임스텝 앞서간다.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 71\n",
            "Number of unique output tokens: 92\n",
            "Max sequence length for inputs: 15\n",
            "Max sequence length for outputs: 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8ajEB69E0pS"
      },
      "source": [
        "## 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKw0yrhlE1kk"
      },
      "source": [
        "# input sequence 정의\n",
        "# Input의 경우 데이터의 모양을 model에 알려주는 역할을 한다. shape, batch_size등이 인수로 들어간다.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "# return_state : 출력과 함께 마지막 상태를 반환할지 여부. True로 하면 state를 2개 가져온다.\n",
        "encoder = keras.layers.LSTM(latent_dim,return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# encoder_outputs는 삭제. \n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "#디코더 설정, 초기상태로 encoder_states 사용\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}